{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from IPython.display import display, Markdown, HTML\n",
    "import jieba\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from preprocessing.document_manager import DocumentManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_constant = 1338\n",
    "\n",
    "doc_manager = DocumentManager(seed_constant=seed_constant)\n",
    "doc_manager.cache_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Sentences Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Sentence:\n",
      "她们都睡了，我蹑手蹑脚摸黑上了床，凑上去想亲嫣一下，她突然一个转身，小手“啪”地搭在了我的脸颊上，我便被施了魔法似地定住了，每次抱着嫣的时候总想让她的小手搂着我的脖子，可她总是不肯，她的两只小手要指挥着我的方向，要指着她感兴趣的东西，一刻也不肯停闲。\n"
     ]
    }
   ],
   "source": [
    "sentences_texts = doc_manager.get_all_sentence_data()\n",
    "print(\"Example Sentence:\\n%s\" % sentences_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Tokenizer from Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.354 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "max_length = 200\n",
    "\n",
    "def cut_text(text):\n",
    "    seg_list = jieba.cut(text, cut_all=False)\n",
    "    new_text = \" \".join(seg_list)\n",
    "    return new_text\n",
    "\n",
    "# Tokenize and join with spaces\n",
    "tokenized_raw_texts = [ cut_text(text) for text in sentences_texts ]\n",
    "\n",
    "# Create and fit Tokenizer\n",
    "def create_tokenizer(tokenized_raw_texts):\n",
    "    input_tokenizer = Tokenizer()\n",
    "    input_tokenizer.fit_on_texts(tokenized_raw_texts)\n",
    "    return input_tokenizer\n",
    "\n",
    "input_tokenizer = create_tokenizer(tokenized_raw_texts)\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "\n",
    "tokenized_input = input_tokenizer.texts_to_sequences(tokenized_raw_texts)\n",
    "padded_input = np.array(pad_sequences(tokenized_input, maxlen=max_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total token count: 53489\n"
     ]
    }
   ],
   "source": [
    "print(\"Total token count: %s\" % len(input_tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words: [('，', 80380), ('的', 54837), ('。', 28038), ('我', 16515), ('了', 14869), ('是', 11138), ('在', 9895), ('你', 6147), ('也', 5545), ('都', 4502), ('有', 4412), ('就', 4354), ('人', 4240), ('他', 4153), ('“', 4141), ('”', 4129), ('不', 3929), ('一个', 3919), ('我们', 3763), ('和', 3692), ('自己', 3465), ('着', 3368), ('、', 3334), ('？', 3130), ('说', 3084), ('她', 2828), ('！', 2757), ('没有', 2735), ('会', 2580), ('让', 2453), ('上', 2423), ('很', 2391), ('这', 2301), ('那', 2149), ('中', 2027), ('去', 2019), ('到', 1952), ('对', 1946), ('要', 1876), ('：', 1803), ('但', 1773), ('又', 1760), ('还', 1755), ('而', 1727), ('…', 1642), ('时候', 1589), ('他们', 1578), ('却', 1536), ('０', 1523), ('这样', 1484)] \n",
      "\n",
      "Somewhat common words: [('第一', 95), ('打电话', 95), ('寻找', 95), ('上班', 95), ('深深', 95), ('比赛', 95), ('忽然', 95), ('夜晚', 95), ('全部', 94), ('满足', 94), ('方面', 94), ('之外', 94), ('利益', 94), ('家长', 94), ('等等', 94), ('正常', 94), ('一颗', 94), ('挺', 94), ('哪怕', 94), ('穿', 94), ('总会', 93), ('考试', 93), ('小时候', 93), ('很大', 93), ('笑容', 93), ('靠', 93), ('失望', 93), ('活着', 93), ('周围', 92), ('一块', 92), ('天气', 92), ('头', 92), ('距离', 92), ('爱人', 92), ('宝宝', 92), ('普通', 91), ('不可', 91), ('过来', 91), ('累', 91), ('理想', 91), ('回答', 91), ('忘', 91), ('而已', 91), ('坚强', 91), ('ｈ', 91), ('安慰', 91), ('失败', 90), ('正是', 90), ('照片', 90), ('获得', 90)] \n",
      "\n",
      "Less common words: [('爱上', 36), ('信心', 36), ('打击', 36), ('感叹', 36), ('抓住', 36), ('尽量', 36), ('留', 36), ('越是', 36), ('美女', 36), ('无声', 36), ('米', 36), ('珍贵', 36), ('依赖', 36), ('所在', 36), ('浪费', 36), ('女朋友', 36), ('绝望', 36), ('日记', 36), ('惬意', 36), ('聊', 36), ('唱歌', 36), ('庆幸', 36), ('商家', 36), ('愉悦', 36), ('流淌', 36), ('相识', 36), ('隔', 36), ('悄悄的', 36), ('花瓣', 36), ('情书', 36), ('元宵', 36), ('奔儿', 36), ('动', 35), ('尊严', 35), ('事实上', 35), ('本人', 35), ('关怀', 35), ('人才', 35), ('今后', 35), ('新浪', 35), ('沟通', 35), ('要说', 35), ('没事', 35), ('缺乏', 35), ('随时', 35), ('脑海', 35), ('早早', 35), ('电视台', 35), ('咱', 35), ('理论', 35)] \n",
      "\n",
      "Uncommon words: [('学有所成', 2), ('身手', 2), ('风尘仆仆', 2), ('晚饭时间', 2), ('酸痛', 2), ('承受力', 2), ('一草一木', 2), ('成瘾', 2), ('媚惑', 2), ('道士', 2), ('插上', 2), ('大丈夫', 2), ('秋霜', 2), ('佛门', 2), ('同心', 2), ('双泪', 2), ('难逃', 2), ('流苏', 2), ('容不下', 2), ('漂染', 2), ('亦可', 2), ('洗头发', 2), ('牢记', 2), ('迷宫', 2), ('管吃管', 2), ('穿成', 2), ('年秋', 2), ('川', 2), ('外婆家', 2), ('合不拢', 2), ('胶布', 2), ('玻璃门', 2), ('独个儿', 2), ('进进出出', 2), ('睁大眼睛', 2), ('宴', 2), ('踏踏', 2), ('马儿', 2), ('反反复复', 2), ('宁', 2), ('护士长', 2), ('克星', 2), ('冒非', 2), ('试管', 2), ('割草', 2), ('侠客', 2), ('抛锚', 2), ('当个', 2), ('为家', 2), ('哪算', 2)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_counts = sorted(input_tokenizer.word_counts.items(), \n",
    "                     key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "print(\"Most common words:\", word_counts[:50], \"\\n\")\n",
    "print(\"Somewhat common words:\", word_counts[800:850], \"\\n\")\n",
    "print(\"Less common words:\", word_counts[2000:2050], \"\\n\")\n",
    "print(\"Uncommon words:\", word_counts[25000:25050], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Save Embedding Matrix for Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4ceec26fa7b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mKERAS_TOKENIZER_PATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_word_embeddings_and_tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/chinese-sentiment-classification/preprocessing/word_embeddings.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tf'"
     ]
    }
   ],
   "source": [
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "\n",
    "from settings.settings import (\n",
    "    EMBEDDING_DATA_ROOT,\n",
    "    RAW_WORD_EMBEDDING_PATH,\n",
    "    EMBEDDING_MATRIX_PATH,\n",
    "    KERAS_TOKENIZER_PATH,\n",
    ")\n",
    "from preprocessing.word_embeddings import save_word_embeddings_and_tokenizer\n",
    "\n",
    "\n",
    "embedding_input_path = RAW_WORD_EMBEDDING_PATH\n",
    "embedding_output_path = EMBEDDING_MATRIX_PATH\n",
    "tokenizer_output_path = KERAS_TOKENIZER_PATH\n",
    "\n",
    "save_word_embeddings_and_tokenizer(\n",
    "    tokenizer=input_tokenizer,\n",
    "    embedding_input_path=embedding_input_path,\n",
    "    embedding_output_path=embedding_output_path,\n",
    "    tokenizer_output_path=tokenizer_output_path,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
