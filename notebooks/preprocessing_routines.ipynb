{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from IPython.display import display, Markdown, HTML\n",
    "import jieba\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from preprocessing.document_manager import DocumentManager\n",
    "from preprocessing.utils import cut_text\n",
    "from settings.settings import PROJECT_SETTINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_manager = DocumentManager()\n",
    "doc_manager.cache_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Sentences Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sentences_texts = doc_manager.get_all_sentence_data()\n",
    "print(\"Example Sentence:\\n%s\" % sentences_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Tokenizer from Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_length = 200\n",
    "\n",
    "# Tokenize and join with spaces\n",
    "tokenized_raw_texts = [ cut_text(text) for text in sentences_texts ]\n",
    "\n",
    "print(tokenized_raw_texts[:10])\n",
    "\n",
    "# Create and fit Tokenizer\n",
    "def create_tokenizer(tokenized_raw_texts):\n",
    "    input_tokenizer = Tokenizer()\n",
    "    input_tokenizer.fit_on_texts(tokenized_raw_texts)\n",
    "    return input_tokenizer\n",
    "\n",
    "input_tokenizer = create_tokenizer(tokenized_raw_texts)\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "\n",
    "tokenized_input = input_tokenizer.texts_to_sequences(tokenized_raw_texts)\n",
    "padded_input = np.array(pad_sequences(tokenized_input, maxlen=max_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Total token count: %s\" % len(input_tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = sorted(input_tokenizer.word_counts.items(), \n",
    "                     key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "print(\"Most common words:\", word_counts[:50], \"\\n\")\n",
    "print(\"Somewhat common words:\", word_counts[800:850], \"\\n\")\n",
    "print(\"Less common words:\", word_counts[2000:2050], \"\\n\")\n",
    "print(\"Uncommon words:\", word_counts[25000:25050], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Save Embedding Matrix for Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "\n",
    "from settings.settings import (\n",
    "    EMBEDDING_DATA_ROOT,\n",
    "    RAW_WORD_EMBEDDING_PATH,\n",
    ")\n",
    "from preprocessing.word_embeddings import save_word_embeddings_and_tokenizer\n",
    "\n",
    "\n",
    "embedding_input_path = RAW_WORD_EMBEDDING_PATH\n",
    "embedding_output_path = PROJECT_SETTINGS.EMBEDDING_MATRIX_PATH\n",
    "tokenizer_output_path = PROJECT_SETTINGS.TOKENIZER_PATH\n",
    "\n",
    "save_word_embeddings_and_tokenizer(\n",
    "    tokenizer=input_tokenizer,\n",
    "    embedding_input_path=embedding_input_path,\n",
    "    embedding_output_path=embedding_output_path,\n",
    "    tokenizer_output_path=tokenizer_output_path,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
