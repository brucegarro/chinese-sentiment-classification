{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from IPython.display import display, Markdown, HTML\n",
    "import jieba\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "from preprocessing.document_manager import DocumentManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_constant = 1338\n",
    "\n",
    "doc_manager = DocumentManager(seed_constant=seed_constant)\n",
    "doc_manager.cache_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Sentences Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Sentence:\n",
      "她们都睡了，我蹑手蹑脚摸黑上了床，凑上去想亲嫣一下，她突然一个转身，小手“啪”地搭在了我的脸颊上，我便被施了魔法似地定住了，每次抱着嫣的时候总想让她的小手搂着我的脖子，可她总是不肯，她的两只小手要指挥着我的方向，要指着她感兴趣的东西，一刻也不肯停闲。\n"
     ]
    }
   ],
   "source": [
    "sentences_texts = doc_manager.get_all_sentence_data()\n",
    "print(\"Example Sentence:\\n%s\" % sentences_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Tokenizer from Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 200\n",
    "max_vocabulary_size = 30000\n",
    "\n",
    "def cut_text(text):\n",
    "    seg_list = jieba.cut(text, cut_all=False)\n",
    "    new_text = \" \".join(seg_list)\n",
    "    return new_text\n",
    "\n",
    "# Tokenize and join with spaces\n",
    "tokenized_raw_texts = [ cut_text(text) for text in sentences_texts ]\n",
    "\n",
    "# Create and fit Tokenizer\n",
    "def create_tokenizer(tokenized_raw_texts):\n",
    "    input_tokenizer = Tokenizer(num_words=max_vocabulary_size)\n",
    "    input_tokenizer.fit_on_texts(tokenized_raw_texts)\n",
    "    return input_tokenizer\n",
    "\n",
    "input_tokenizer = create_tokenizer(tokenized_raw_texts)\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "\n",
    "tokenized_input = input_tokenizer.texts_to_sequences(tokenized_raw_texts)\n",
    "padded_input = np.array(pad_sequences(tokenized_input, maxlen=maxLength))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53489"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_tokenizer.word_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
